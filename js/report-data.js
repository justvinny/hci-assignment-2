// Mock Data
const SECTIONS = [
  {
    id: "usability-research",
    content: `<p>
    From the perspective of the software developer, usability is primarily concerned with the quality of the user’s experience while using a piece of software. It is a way to evaluate whether a product meets measurable usability metrics and if the user can perform desired tasks quickly and efficiently. (Hughes, 1999) Highly usable products, therefore, will lead to a “lack of frustration” for the individuals using them. (Rubin & Chisnell, 2008) Indeed, even before the advent of cutting-edge information technology tools, products with poor usability have led to potentially disastrous consequences. For instance, a usability issue was attributed as the reason for a miscount of votes in the 2000 United States presidential election within Palm Beach County, Florida, where officials fatally chose to use a ballot with rows that appeared misaligned to many voters. This seemingly insignificant error caused a surplus of 2,000 votes to be erroneously counted for the Reform Party ticket instead of their Democratic counterparts. (Hertzum, 2020; Wand et al., 2001) This error proved ruinous for the Democratic ticket, as candidate Al Gore famously lost the 2000 presidential race in Florida to George Bush by a margin of less than 600 votes.
    <br><br>It is critical, therefore, to thoroughly test the usability of all products before their release into the general market. The basis of a usability test has existed for several decades, with its early proponents referring to it as a guided verbalization technique, or “thinking-aloud” method, wherein a moderator encourages a participant to actively vocalise their thoughts on a subject. Ericsson and Simon were two of the first scholars to theorise on the importance and validity of this technique, primarily in their 1984 book <em>Protocol Analysis: Verbal Reports as Data</em>. Within this work, it was posited that certain verbalizations of a test subject, such as objective descriptions or reports of an activity, image, or abstract concept, can be treated as valid data. (Ericsson & Simon, 1993) Though the findings of Ericsson and Simon, who were a political scientist and psychologist respectively, were specific to the field of cognitive psychology, they laid the foundation for an interdisciplinary body of work from a variety of other researchers.  One of the most significant impacts of Ericsson and Simon’s book has been on the field of usability testing, which emerged shortly thereafter on a wider academic and procedural scale. During a modern usability test, participants will be encouraged to continuously vocalise their thoughts, a practice which was pioneered in <em>Protocol Analysis</em>.
    <br><br>However, modern practices of usability testing diverge significantly from the findings of <em>Protocol Analysis</em>, as Ericsson and Simon did not believe expressions of subjective opinion were of much use to psychological practitioners. In addition, they favoured a hands-off approach when it came to guiding participants’ thought processes, with only occasionally reminding them to keep talking. (Boren & Ramey, 2000) Today’s usability testing is more flexible, but the concept of moderator intervention remains highly debated within academic circles. Whiteside and colleagues (1988) argued that if a moderator simply observes a participant’s behaviour and takes notes of objective fact, they are likely to form their own conclusions about the user’s feelings on the subject of the usability test. This would effectively detract from the competence of the study, as the degree of user satisfaction might not be sufficiently recorded. (Tamler, 1998) Whiteside et al. also concluded that the act of merely observing a participant reduces their role in the study to a scrutinised object, which may also affect their attitude and subsequent performance.
    <br><br>Generally, most usability researchers will agree that the context and purpose of a usability test will determine the behaviour of both the participant and the moderator, which can vary greatly. The goal of usability testing is most often to collect qualitative data, which is effectively gathered when the participant is actively guided through using the product. Therefore, there are certain types of moderator interference which are critical to the success of the study, such as asking questions which are designed to elicit a verbalised response, listening actively, and guiding the user. The activities of the study often unfold in such a way that participants may naturally be inclined to make suggestions on how to improve the usability of the product.   
    <br><br>A usability test does not exist in a vacuum. The conduction of a usability test is always preceded by research and methodological preparation. There is always the underlying assumption that the moderator has some fundamental knowledge of the product being tested. (Rubin & Chisnell, 2008) A usability test is also followed by rigorous analysis of its events, and, if applicable, the implementation of its findings to create an updated, superior product.  This can be viewed as an element of the Wheel model, as detailed by Hartson and Pyla (2018) in their book about UX design. The Wheel model is a blueprint for the design process, which describes the cyclical nature of software development, wherein a new product is designed, then implemented, then evaluated and analysed. The Wheel model is, of course, formed in a circular shape, implying that the last step feeds directly into the first one, keeping the design process in a perpetual cycle of development and redevelopment.
    </p>
    <br><img src="assets/software-dev-lifecycle.png" class="align-self-center header-image-size" data-aos="zoom-in" alt="Figure 1: Wheel model of software development cycle, Hartson & Pyla (2012)"/>
    <br><em>Figure 1: Wheel model of software development cycle, Hartson & Pyla (2012)</em>
    <p>
    <br><br>When it comes to the number of participants in a usability study, less is more. This is proven by the work of Neilsen and Landauer (1993), who designed a logarithmic curve which endeavoured to find the optimal number of participants in a usability study. The base of the curve reveals one fundamental truth: zero participants will always give zero usability insights. A single participant is likely to provide practitioners with roughly 1/3 of their total knowledge. By the time five or six participants have been interviewed, moderators will have gained an understanding of up to 90% of total usability problems. Increasing the number of participants any further runs the risk of redundancy, and conductors of a usability test might find themselves hearing the same comments repeatedly by several participants. (Nielsen, 2000; Nielsen & Landauer, 1993) This is especially pertinent in cases where tasks are specifically designed for participants to expose potential usability issues, where the problem is already predefined. Within our usability study of the Canvas mobile application, we chose to have six participants, which proved to be the perfect number for our purposes. We did find some repetition within participants’ comments towards the end of our study; however, each new participant still supplied us with some sort of differing perspective.
    </p>
    <br><img src="assets/optimal-participants-graph.png" class="align-self-center header-image-size" data-aos="zoom-in" alt="Figure 2: Nielsen and Landauer's curve showing optimal number of participants"/>
    <br><em>Figure 2: Nielsen and Landauer's curve showing optimal number of participants</em>
    <p>
    <br><br>According to Barnum (2010), in the book <em>Usability Testing Essentials</em>, the best way for practitioners to implement a small usability study is to follow general guidelines which include defining the user group, using a think-aloud method, and creating task-based scenarios for participants to complete. Within the usability test our team conducted for this project, it was simple to define the user profile: any AUT student who uses Canvas, whether or not they had experience with the mobile application. Our pool of participants ended up being quite diverse regarding their prior knowledge of the app, with some admitting they have never used it before, and others being active, daily users. We enforced a think-aloud method, asking participants to actively voice their thoughts as they used the Canvas application. Though most participants were willing to continuously share their line of thinking, those who were more reticent were prompted to speak up with non-leading questions such as, “What is your impression of that?” The team also made a list of tasks or prompts for participants to complete. These were introduced after participants were initially asked, “What would you normally use the Canvas app for?” These questions served to guide the participant but attempted to remain as unobtrusive as possible.
    <br><br>Despite the ever-increasing popularity of mobile applications, there are not universally agreed-upon metrics for their specific usability. Many app usability tests will use more generalised standards to measure how usable a product might be, such as those laid out by the International Organization for Standardization in the ISO 9241-210:2019, which is not tailored to mobile applications. (International Organization for Standardization, 2019) Hoehle and Ventakash (2015) took this absence of mobile-specific standards to create a list of twenty attributes that a highly usable application should have. These were adopted with reference to Apple’s general UX guidelines, which are considered reliable as the App store is the largest distributor of mobile applications. They included but were not limited to realism, effort minimization, concise language, logical path, aesthetic graphics, and control obviousness. For our purposes, we were mainly concerned with the aspects of logical path, control obviousness, and effort minimization, which collectively refer to the ease with which a user can navigate through and successfully use an application component. Our given tasks were primarily aimed at testing the usability of the Canvas application through these perspectives.
    <br><br>In recent decades, during the rapid expansion of information technology, there has been much academic review and literature concerning the usability of IT products. Across academic sources, there seems to be a general consensus on how to conduct a good usability test: employ a thinking-aloud method, intervene an appropriate amount, choose 5-6 participants, and assign them tasks to complete. Within our own work, we utilised these techniques thoroughly and thus ensured the practical relevance and robustness of our usability study.    
    </p>`,
  },
  {
    id: "method",
    content: `<p>
    For our study we researched multiple usability testing methods to get the most meaningful feedback from the users. We looked into techniques such as "thinking aloud" and moderated testing. The "thinking aloud" method is a method which encourages the user to speak their mind while using the app. Moderated testing is a form of testing where the observants guide the participants throughout the study. We found these methods best suited our study.
    <br><br>The "thinking aloud" method is a usability method where the user exercises the product while saying what comes to their minds. In our case the users would be using the canvas mobile app while thinking aloud. The people conducting the test then observe what the user is saying, giving insight into what they think of the app. Before our testing we would actively encourage the participants to say what came to their mind. We found that asking before rather than during the test was most effective as it provided less distractions for the participants. We also found that building a rapport with the participants was necessary as they felt more inclined to answer honestly. 
    <br><br>We also chose to conduct a moderated usability test rather than an unmoderated test. Moderated testing involves being present with the participants throughout the study to collect feedback along the way.  In our test we would provide a set of tasks that would help with creating an objective for the participant. We found that if there wasn't a set of tasks to do then there wouldn't be enough content to observe as the test was too short to gather any evidence. A moderator also has the ability to make the participant more engaged when doing the tasks, leading to more reliable data.
    <br><br>After choosing our techniques, we looked into forming the actual steps for the study. We decided to use these steps after looking through other usability tests and studying the basic components that these tests were made up of.  
    <br><br><strong>Developing the objectives for the test</strong>
    <br>We decided on four tasks the participants would try and attempt to complete. The tasks were toggling on dark mode, adding an event to the calendar, customising the layout of the course dashboard and accessing the discussion forum. We chose these tasks as they best represented what ordinarily be performed during the use of the product (Rubin et al., 2008 ). 
    <br><br><strong>Choosing participants for the test</strong>
    <br>For choosing the participants we decided to select 6 random students. To make sure our results were valid we made sure to pick people that were typical users of the product as not doing so could yield poor or invalid results (Rubin et al., 2008 ). We would make sure to select students who used canvas as their main course management system rather than other programs such as Blackboard. To reduce any chance of bias we also chose students who were alone rather than students who were in a group.
    <br><br><strong>Representing the actual work environment</strong>
    <br>To represent the actual environment in which the app would be used in, we would acquire a room that simulated an environment people would use the canvas app in. We decided on field testing rather than a normal lab environment because it provided the actual context of use of the product, context like the actual work environment (Barnum, 2010). We would also let the participants decide whether they wanted to use their own device or a device that we would provide. One aspect that we kept constant was the operating system of the device, we would use IOS on all tests to reduce any inconsistencies that could arise from performance differences between Android and IOS devices. 
    <br><br><strong>Analysis of data</strong>
    <br>We would collect information from the participants based on their interaction with the app. After collecting the information we would begin analysis of the data. The data would be analysed both individually and merged together. Individual analysis of the participants serves to turn the participants' behaviour into the actual usability findings while the merging of data can provide information such as frequency (Hertzum, 2020). 
    <br><br><strong><u class="sub-header-font font-space-blue">The usability testing had multiple stages of engagement with the canvas app.</u></strong>
    <br>At first we allowed the participants to play around with and explore the app on their own for a couple of minutes so that we could gain a better understanding of how they use the app. We let them do so until they were comfortable enough to start carrying out the tasks.
    <br><br>We then gave them a set of tasks to perform and look into, these tasks included toggling dark mode on, adding an event to the calendar, changing the layout and colour of the course  dashboard and accessing the discussion forum.
    <br><br>For our first task, we asked the users to try and toggle on dark mode through the app. The participants were instructed to go and find the settings to try and toggle dark mode for the app. We would remind the participants to speak whatever came to their mind while doing the task. We would then note down anything the participants had to say about the task along with our observations.  
    <br><br>For our second task, we asked the users to add an event to the calendar. The participants were instructed to navigate to the calendar then add any event to the calendar with any date they chose. The participants would say what they thought of the task while they were doing it. After this we would note down any quotes from the user and the way they carried out the task. 
    <br><br>For our third task, we asked the users to try to change the layout of their dashboard. The users would go into the canvas course dashboard and were instructed to try and change the name, layout and the colour of the courses. We encouraged the participants to say what came to their mind during the duration of the task. After this we would note down any thoughts from the user and how they performed the task.
    <br><br>For our fourth task, we asked the participants to try and access the discussion forum. The participants would go from the dashboard and try to navigate to the discussion forum. We made sure to remind them before the task to verbalise what they were thinking of while doing it. We would note any verbalization from the participant and observations on how they performed the task 
    <br><br><strong><u class="sub-header-font font-space-blue">Steps we took with our participants</u></strong>
    <br><strong>Gathering the subject</strong>
    <br>To gather a good range of participants we made sure to select random students around the WG building, we made sure to get participants that were alone to reduce influence that each participant may have had on eachother. We did this so that there was less chance of bias in our observations. 
    <br><br><strong>Getting acquainted/making them feel comfortable</strong>
    <br>Once we were able to get the participant, we made sure to make them feel comfortable and built a rapport with them. We did this so they felt comfortable enough to be genuine and honest with their reactions. 
    <br><br><strong>During the test</strong>
    <br>During the actual test we made sure to let the participant know that there was no wrong answer or no wrong way to carry out the tasks. We did this so that the participants wouldn't feel like they weren't doing anything wrong and so that they wouldn't be guided to a "right" or "wrong" answer. We would also encourage them to verbalise while doing the tasks.  
    <br><br><strong>After the test</strong>
    <br>After the test we made sure to reward the participants for taking part in our study. We wanted to thank and let the participants know that we appreciated them for their time. We ended up rewarding them with some sweets.
    <br><br><strong><u class="sub-header-font font-space-blue">What we noted down</u></strong>
    <br>For our notes and observations we decided to categorise it based on the tasks we had chosen. We also decided to add extra categories for the body language of the participants and any notable opinions and suggestions they had for the app.
    <br><br><strong>Free observation</strong>
    <br>We noted down things such as body language while using the app, how well they navigated through the app, any notable thing they said such as what they found nice or what they found annoying about the app.
    <br><br><strong>Toggling dark mode</strong>
    <br>We would note down how easily they were able to toggle dark mode or how difficult they found it. We would observe and note down their body language while doing so. We also noted down anything they had to say such as whether they found it easy to toggle it on or off.
    <br><br><strong>Accessing discussion forum</strong>
    <br>We noted down how easily they were able to access the discussion forum. We would look at things such as how quickly it took them to access it or how long they took. We would note down anything they said while accessing the discussion forum.
    <br><br><strong>Dashboard customization</strong>
    <br>We would look at things such as if they were able to find the settings for the dashboard customization. We would note down how easy or difficult it seemed for them to find the setting. We would also note down anything they said about the experience such as whether they found it intuitive or not.
    <br><br><strong>Adding event to calendar</strong>
    <br>We would look at if they were able to find the calendar feature for the app, we would also observe if they were able to add an event without any trouble or if they found it difficult to add an event. We would also note down anything they had to say such as whether they found it easy or difficult to add an event.
    <br><br><strong>Body language</strong>
    <br>We also made sure to note down the body language of the participants. We would observe them while using the app and see if they were comfortable while doing the tasks. We would look at their hands to see if they knew where to go or if they didn't. We would observe whether there were any long pauses or if they were able to immediately do the task.
    <br><br><strong>Participants thoughts</strong>
    <br>We had to make sure to note down whatever the participants said during the duration of the study, the participants would "think aloud" and say things such as whether they found a task easy or not. We would also note down other things such as any suggestions they had towards any of the apps features.  
  </p>`,
  },
  {
    id: "analysis-and-conclusions",
    content: `<p>
    There are two major methods of research and analysis: quantitative research and qualitative research. Gray and Densten (1998) note that quantitative research is often based on numerical data and facts, following closely to the scientific method and the ability to recreate results. For example, Segtnan and Isaksson (2000) gathered measurements when investigating the carbohydrates found in fruit juices. This data was numerical and entirely objective, and so they performed a quantitative analysis. Gray and Densten (1998) describe qualitative research as a more naturalistic process that focuses on real world implications and interpretations of text-based information. For example, Ezzy (2000) gathered data about people living with HIV/AIDS through conversational interviews. Because this data was gathered verbally and later transcribed to text form, Ezzy carried out a qualitative analysis of the data. Our usability study aimed to form insights about how usable the Canvas mobile application is for AUT students, and form suggestions for future development to improve the application. Because our study was designed to form insights from discussions and interpretations of our participants’ actions, a qualitative approach is most appropriate.
    <br><br>There are many different types of qualitative analysis methods that can be carried out, the main studies being ethnography, critical social theory, content analysis, narrative analysis, and phenomenology (Cirgin Elliot & Beausang, 2002). Ethnography is often claimed to be the first qualitative method, based not in asking why a social group is formed, but instead on giving an accurate description of the group’s life (Peacock, 1986). Critical social theory often targets oppressed social groups and aims to bring attention and understanding to these communities (Crossley, 2005). Content analysis is used to find themes from almost any type of data, whether it be conversational, story based, media, etc., and identify expectations and perceptions (Miles & Huberman, 1994). Narrative analysis, like the name suggests, is used for analysing accounts of events to find meaning in the stories (Ezzy, 2003). Phenomenology can be split into grounded theory and hermeneutics. Grounded theory is the use of participant data to discover themes which can confirm or deny pre-established theory(s) for a phenomenon (Ezzy, 2003). Hermeneutics is used to blend together a researcher’s ideas of a phenomenon with data from particpants and outside relevant data to form an understanding of the phenomenon itself (Wojnar, & Swanson, 2007). Most of these studies require a pre-existing theory to be confirmed through the analysis of data. Our usability test was a purely exploratory study, so we decided to use a slightly different analysis method: thematic analysis.
    <br><br>Thematic analysis can be described as a mix of the qualitative methods of grounded theory, positivism, interpretivism, and phenomenology (Guest, MacQueen, & Namey, 2012). It aims to discover codes (themes) throughout a dataset to inform real solutions or build models. While it has the limitation of potentially missing more subtle or nuanced observations, it works well for a group of multiple analysts and is rooted in combining the best parts of multiple accurate methods. Thematic analysis works well in exploratory situations when planning begins before the data collection (Guest, MacQueen, & Namey, 2012). This planning can involve strategies on how to ask participants about their experiences or how to prompt them to further explain. The nature of our usability study meant that we already did this planning. It also involves having a general idea of the categories the study may be broken into, but these would be general ideas like ‘how does dark mode affect users?’ rather than very specific issues like ‘users cannot read buttons in dark mode’ (Ezzy, 2003). These categories are later split into specific codes that can be used to draw conclusions.
    <br><br>The usability study was split into two distinct sections: the exploratory section and the guided tasks. In accordance with the thematic analysis method, we entered into the study with a few general issues in mind which we used to form our guided tasks, but had no specific features pinpointed. This meant that we could code our data and find the common themes among participants during and after the sessions. To sum up the coding, three main themes were identified in the exploratory session (and confirmed in the guided section): participants use the website much more regularly than the application, the application is used mostly for quick actions like checking timetables or assignment due dates, and the layout of the application can be confusing and messy within each paper’s section. We have concluded that the lack of use of the application is due to the lack of consistency in between course layouts as well as the application being too similar to the website. By this, we mean that while the app looks very close to the website’s layout, it is not suited to smartphones as the sections, for example the ‘Course Overview’, ‘Teaching Team’, and ‘Assessments’ when users enter the ‘Home’ section of a paper, are too big on the screen and suited more computers than smartphones. The lack of use is also due to how students are used to studying. A student is much more likely to watch lectures or read through slideshows on a larger screen like a laptop than a small smartphone screen because the content will be bigger. This is what leads to students using the application mostly for quick actions such as checking their timetables. The layout issues are not exclusive to the application because course leaders are given the ability to edit their course pages individually. However, this leads to a lack of consistency for students and can make finding specific content difficult with Canvas.
    <br><br>The guided tasks were all carried out in dark mode (with the exception of the first task to find dark mode), and hence revealed insights about the nature of dark mode in Canvas. An overwhelming theme within dark mode was that buttons were extremely difficult to find because their colouring was not adjusted for the dark background. This meant that there were dark grey buttons on a dark grey background, and some participants had to fully close the application because they could not exit the section. There were also some issues with where some features could be found. Most participants expected the features to rename and recolour courses to be found in the ‘Edit Dashboard’ section, when it was actually found in the meatball menus of each course. The option to switch between dark mode and light mode was also expected to be in the hamburger menu, with some participants trying first to change the colour overlay option, instead of inside the settings within the hamburger menu.
    <br><br>This usability test and the analysis we conducted helped us to develop some suggestions for improvements to the Canvas mobile application. First, we suggest that dark mode be further tested and improved by adjusting buttons to be white so they can be seen on the dark background. Next, we suggest that features be moved to more intuitive sections. Any change of the dashboard, whether it be the courses displayed, the course names, the course colours, the course orders, etc. should all come under the ‘Edit Dashboard’ section. Further, we suggest that the ‘Edit Dashboard’ section itself be moved into the hamburger menu. The course specific features (colour and nickname) can also stay in the meatball menus as they relate to each course but having them available in the ‘Edit Dashboard’ section may make it more intuitive to find and use. Within the hamburger menu, we suggest that all settings be moved out of the settings section and instead have the ’Preferences’ and ‘Legal’ sections be within the hamburger. The ‘Show marks’ and ‘Colour overlay’ could then be moved into ‘Preferences’, as ‘Options’ is the same thing. The course layouts should also be edited to be more mobile friendly, particularly the ‘Course Overview’, ‘Teaching Team’, and ‘Assessments’ sections found in every course. These should be smaller, perhaps just bars across the screen rather than large sections that don’t all fit on the screen. We also advise that Canvas offer a more uniform way of setting up courses so that they have a more consistent layout for students.
  </p>`,
  },
  {
    id: "individual-reflections",
    content: `
  <div class="mr-auto" data-aos="zoom-out">
    <img
      class="person-avatar person-female"
      src="assets/undraw_female_avatar.svg"
      height="100px"
      alt="Photo of Abby"
    />
    <span><strong class="sub-header-font ml-2">Abby Laloli</strong></span>
  </div>
  <p>
  <br>Overall, I found this usability study an interesting and enlightening experience. Before the study, I started my research by reading over the sources provided in class: lecture slides and notes I had made, as well as the recommended reading of Liberating Usability Testing by Phil Carter. This gave me a good general idea of what a usability study is and how to conduct it. As a group, we made an effort to do individual research on usability studies and then come back together to brainstorm what we had learnt. This meant that we could discuss what we had learnt and clarify anything we didn’t understand fully or had misinterpreted. For my research, I looked into usability study examples because I already had a good base knowledge of how to complete one and wanted to see how others had approached it already. Through my research I found that some sources specifically said not to guide participants in their actions and others said that we should give participants specific guided tasks. When we came back together as a group, it seemed that everyone had this issue, so we decided to make our usability study a blend of both techniques. I also noted that we should have some way of going back and looking at the raw ‘data’ from the sessions, as usability studies can be extremely subjective and interpreted differently by different people. This is why we decided to video the hands and screen of each participant. I learnt about how to analyse the study in a similar way, by first looking at any examples from our classes and then doing my own independent research based on what I had found.
  <br><br>While conducting the usability study I learnt a lot about the nature of these studies. One of the main themes I noticed throughout my research was the emphasis on forming a rapport with the participants and keeping them comfortable with us as researchers. Before the study I didn’t think this would be very difficult as I consider myself personable, but I hadn’t considered that the participants themselves might just be a little uncomfortable in the situation. I found that most of the participants had trouble ‘thinking aloud’ as we had asked them to, and so we had to ask them questions about what they were feeling and thinking which disrupted the flow of the study. I also found it very interesting that some features we were expecting them to struggle with were found easy, while others we expected to be easier were difficult. When analysing the data, I learnt that it was a much longer process than I had anticipated. While we didn’t fully code the data as is the nature of most thematic analyses, it was a lengthy process to find the themes and decide on them. It was also difficult to look at the ‘data’ in a subjective light as I already have my own opinions on the features of the Canvas application.
  <br><br>If I could conduct this study again, I would do a few things differently. First, I would do more research on and practice how to build up a connection with the participants and how to ask questions without accidentally leading them towards an answer. I thought that this wouldn’t be difficult and would come naturally, but once we conducted the study, I saw that it is quite a nuanced skill. I would learn to do this by practising questions on people and also looking into specific language to use to lower my influence as a researcher. I think that changing the scenario in which we conducted the study would help this, as the small study room we used and the way we sat the participants at the head of a table made it feel more formal and like an interview than a comfortable space. In the future I would also like to look more into and clarify how to guide participants in the exploratory section. I think this would have revealed more organic thoughts about the nature of the Canvas application, but we were simply not skilled enough to carry out this section properly. Finally, I would have liked to have had the time and opportunity to do a deeper analysis into the data we collected. I find usability and human computer interaction to be extremely interesting topics so I would have loved to look into things like exactly why finding the hamburger menu in the top right corner is more intuitive than the bottom left, or why people prefer things to be laid out differently on smaller screens. This study has taught me to be more aware in the programs that I develop of the importance of making every action as intuitive as possible, so users have a positive experience. These skills and this new awareness will be helpful for me in industry, as I now have the foundation on which I can build even more practical knowledge surrounding human-computer interactions.
  </p>

  <br><span class="anchor" id="reflection-kat-anchor"></span>
  <div class="mr-auto" data-aos="zoom-out">
    <img
      class="person-avatar person-female"
      src="assets/undraw_female_avatar.svg"
      height="100px"
      alt="Photo of Kat"
    />
    <span><strong class="sub-header-font ml-2">Kat Millicevic</strong></span>
  </div>
  <p>
  <br>This group project brought me out of my comfort zone quite a bit. I am typically rather reserved with my classmates, but I had to reach out to multiple strangers to recruit participants for our study. This was especially true when the vast majority of people we approached on campus said no and seemed hostile, as if we had wasted their time. I have also never been in the position of someone conducting a study or experiment of any sort, but I feel that I adapted to the role quickly on the day that we conducted our study. I found the group we worked in to be harmonious and understanding of each other, especially during the end-of-semester rush for everyone to hand in coinciding assignments on time.
  <br><br>Regarding the academic content of our assignment, I believe it proved to be surprisingly easy to come up with a list of tasks we would assign our participants, after we had collectively come up with the topic. We came up with our list of usability issues organically, as they naturally occurred to us in group conversation. For instance, one of our team members, Vinson, who uses his phone on dark mode, noted that certain features are completely invisible on the Canvas app for him due to poor color grading. Conversely, when I noticed that Abby, another team member, had set nicknames for her classes, I asked her how to do it and found I personally struggled with finding the path to this function within the app. We ended up using both of these usability issues to formulate tasks for our participants in the study. My only qualm with the way our study unfolded is that I felt I had not done enough personal research prior to beginning the usability test. I was satisfied with the end result of our task list for the participants, but I personally feel anxious going into any sort of assignment without having completed an immense amount of prior academic research. We were all increasingly aware of a looming assignment deadline and felt we had to move on with our test as quickly as possible, so we would have time to finish our report, reflections, and a functioning static website to host the contents of our assignment. Overall, I felt we all played the role of usability practitioner well, and I was pleased with how we managed to convey a professional but relaxed image to our participants. I believe this environment put them at ease but made them very willing to help our study.
  <br><br>Prior to taking this class, I had never really considered the issue of human-computer interaction, and I accepted usability issues as fact. Though I aim to avoid them in programs I construct in my studies, I felt they were inevitable as poor development is highly prevalent with such a high volume of websites, applications, and software in circulation today. I also assumed that usability testing would be a highly technical, formal procedure where strict controls are enforced. Upon learning that it is much more dynamic and situationally dependent, I can confidently say that I will be attempting to implement the concepts of usability testing within my future career in information technology. The core aspects of usability testing are quite logical, with their underlying drivers being to simply execute the human-centric design of products. This means that the fundamentals of usability will always be embedded into my work, as the perspective of human-computer interaction is valuable and pervasive in all technological design.
  </p>

  <br><span class="anchor" id="reflection-sungwoo-anchor"></span>
  <div class="mr-auto" data-aos="zoom-out">
    <img
      class="person-avatar person-male"
      src="assets/undraw_male_avatar.svg"
      height="100px"
      alt="Photo of Sungwoo"
    />
    <span><strong class="sub-header-font ml-2">Sungwoo Cho</strong></span>
  </div>
  <p>
  <br>On the surface the canvas app appeared to function well as a course management system however upon further evaluation we found that some features of canvas were not as intuitive as they seemed to be. We conducted a usability test to see how well the average user would be able to navigate through the app, gathering results in the process. For the test we had the participants complete a series of tasks we compiled. We chose these tasks as we thought they best represented what most people would have used canvas for. However while conducting the actual study we had a few participants saying that they had never used some of these features. Another group member and I had the role of observing the participants and taking notes while they used the app. After recording the results we began to give roles for everyone and plan how to write our study. I personally did not take much initiative when deciding what role to undertake for the write up of the study, instead having my role assigned by one of my group members. During team meetings I also found that I wasn't being vocal enough when taking part in group meetings.
  <br><br>While reflecting on the study, I learnt more about how they function and some areas where I could improve. We thought the features we had chosen were the most used however we soon came to the realization that this wasn't the case. One feature that we noticed that rarely got any use was the calendar app, our group used this feature on a regular basis so we decided to add it to the list of tasks. We may have used the calendar regularly but we didn't consider whether other people saw use of it, instead just assuming that they did. I learnt that while I did contribute to the discussions it wasn't enough. I feel that if I was more vocal it would have led to more engaging discussions which could have positively benefited the quality of the study. As a result of this, I've learnt how important it is to be more vocal while taking part in group discussions. I've also learnt how crucial it is to take more initiative when it comes to choosing roles or what parts of the study to work on. Although I was lucky enough to be given a part which I was happy to do, in future studies I could be given a role which I am uncomfortable with. In order to work on content that I am confident with I should have taken more initiative choosing my role and expressing my interest in undertaking certain parts of the study. 
  <br><br>For future studies I would apply what I learned during the usability testing to overcome the issues I faced. I would gather more information from the demographic we are studying before conducting the test to get a better sense of what people think rather than just limiting it to our group members. I would do this by asking or surveying a wider audience before deciding on what tasks to include in the study. I think that doing so would allow for a more accurate representation of what people would use the app or product for, resulting in better quality results. I will ensure that I increase how vocal I am during discussions and meetings to allow me to be a more effective teammate. In order to do this I will need to be more engaged during discussions and ask questions whenever I am unsure of anything. Doing so would make the content that comes from these discussions of higher quality. I would also make sure to take more initiative when roles are given out. Next time I will make sure to voice my opinion if there is something that I am not confident in working. To do this I will say what I would be confident to work on instead of just waiting for something to be assigned to me.
  </p>

  <br><span class="anchor" id="reflection-vinson-anchor"></span>
  <div class="mr-auto" data-aos="zoom-out">
    <img
      class="person-avatar person-male"
      src="assets/undraw_male_avatar.svg"
      height="100px"
      alt="Photo of Vinson"
    />
    <span><strong class="sub-header-font ml-2">Vinson Beduya</strong></span>
  </div>
  <p>
  <br>This is a reflective essay about the usability study we conducted for the Canvas mobile application. I will first give a brief background about myself and the reason why we chose Canvas as our topic. Then I will discuss the overall process of how we conducted the usability study and my role in it.  After which, I will reflect on my experience with the project and the things I have learned thanks to it. Finally, I will then analyse how this would be helpful to my career as a Software Developer in the real world. 
  <br><br>Firstly, some background about me—I am a career changer who discovered his passion for programming in his mid-twenties. My programming experience has been very brief at only a meagre three and a half years. There is definitely still a lot for me to learn, but I am quite happy to have been blessed with multiple opportunities the past year or two that helped me break into the industry as early as my 2nd year of University. This was a very exciting phase of my life which proved my hard work was not in vain. I was able to demonstrate my passion for programming through multiple personal projects in my free time which led to an offer for a role as a Software Engineering Intern at Xero last summer. That internship allowed me to learn a lot about real world Software Development and gave me the opportunity to work with a lot of brilliant minds. Not to mention, this experience over the summer further reaffirmed my decision to become a Software Developer and am eagerly looking forward to getting back to work in the industry after this semester ends. 
  <br><br>Secondly, the reason why we chose to conduct a usability study on the Canvas mobile application was very simple—all members in the team are very interested in having a career in Software Development. This is why it easily made sense to choose a project closely related to that. In the initial phases of the study, each of us did our individual research on how to perform a usability study. In my research particularly, I was focused on studying two sources I found—a course about usability studies by Google as part of their User Experience (UX) design certification program and a usability study from Portland State University on their library’s mobile website. I really liked the course from Google as it provided very thorough examples of both moderated and unmoderated usability studies. I learned the importance of mitigating bias forming with our participants and the important things to look out for when taking notes. I then understood how to properly observe participants based on their emotions, body language, reaction, impactful quotes, and feedback. As for the study from Portland State University, it provided me with a more academic perspective on how it is performed compared to Google’s course as it focused a lot more on theory and referencing. Both have definitely been helpful in improving my understanding on the topic. Moreover, the things I learned from these are consistent with my experience working with professional UX designers from Xero and how they conducted usability studies with their users. It was definitely interesting to gain more understanding of what a UX designer does as it requires a totally different skill-set compared to Software Development. After which, we convened with the team and combined our individual findings to determine the process for our usability study. 
  <br><br>Thirdly, during the actual study itself, my main roles were acquiring participants and note taking. I leveraged two channels to find our participants—through our Discord network and cold approaching random students at the University. The first approach was definitely easier, effort-wise, as all it took was a single message online. Unfortunately, we were only able to gather four people using this method due to a lot of schedule conflicts and were still missing two more participants. This is why we decided that Kat and I would be approaching random students at the University. This definitely took a lot more effort as not everyone was exactly keen to participate in a random study especially since we were not allowed to entice people with gifts or incentives. Fortunately for us, we were able to find two more people in the end after talking to dozens of students. As for the gifts, we did have some chocolates and lollies, but we only gave them out at the end and never mentioned them before due to ethical implications that it would be perceived as a “bribe”. As for the note taking, it was Sungwoo and I who took the notes. Whereas, Abby moderated the study and asked follow-up questions; and Kat was tasked in recording the participant’s interaction with the application.
  <br><br>Fourthly, after the study concluded, I was tasked with designing and developing the digital portfolio as I had the strongest technical knowledge in programming within the team. Since the portfolio is simply a static site which does not need to communicate with a backend, I decided to simply use HTML, CSS, JS, jQuery, and Bootstrap instead of relying on a frontend framework like React. This is because using React would be overkill and would bloat the application with too many unnecessary external dependencies. As for the design, it is inspired from a lot of modern websites and the colours are consistent with Material UI guidelines. The Material UI guidelines ensure that programmers are able to consistently deliver professional looking applications even without a design background. As for the user experience of the application, it was designed with ease of use as the focus and I constantly put myself in the shoes of a user. I made sure that the website was responsive to not only desktop computers and laptops but also to a mobile phone. Moreover, since this site’s purpose involves a lot of reading, I have added two features which enhance that aspect for the users by allowing them to change the font family and the font size. I also took into account dyslexic readers so I have added the option to choose a font which is dyslexia friendly. Another thing I took into consideration was blue light exposure causing eye strain, so I added the option to toggle dark mode for the site. Furthermore, all of these settings are persistently stored in the browser cache so it will remember the user’s preferences for fonts, size, and theme. This way they will not have to keep reconfiguring the site every time they reload the page. I have also added easy navigation through a stickied toolbar which uses scrolling animations to inform the user concisely where they are moving to within the page. As for ensuring it is highly accessible by visually impaired readers, I have added labels for all relevant images and non-text HTML elements.  Finally, to avoid cluttering the browser with all the extra UI features I added, I create a visibility toggle so users can show and hide these at their convenience. This is especially helpful on mobile devices which do not have much screen real estate. 
  <br><br>Overall, I found this assignment to be quite educational and I love how it gave me extra insights on the work of User Experience (UX) designers and researchers who I will be working closely with throughout my career as a Software Developer. This was definitely one of the more practical assessments for the course and that way of learning resonates with me more as I am a hands-on and visual learner. This learning style is why I got into programming in the first place as I loved being able to build things and learn at the same time using code. I especially love the dopamine high I get from solving problems in a Software Development context. As for my weaknesses, one would definitely be academic research as it does not appeal to me at all and the other would be my lack of experience in a leadership position. For the latter, I am confident that I will improve on that aspect given enough time and the opportunity to learn from successful leaders within the industry. In saying that, for the immediate future, my main focus should be on the Software Development side of things as that field alone already has so much things I need to learn about. The Computer Science field is just too vast and technology evolves at a rapid rate that it is simply impossible to learn everything about it even with a lifetime of learning. This means that I should not be diverting my attention by learning topics unrelated to the field I want to be in as that could potentially hinder my efforts in becoming truly good at my craft. Therefore, it is best for me to take things one step at a time and hyperfocus on the more immediate skills necessary for Software Development until I reach my goal of being able to design and write robust software that scales with minimal issues to an enterprise level.
  </p>`,
  },
  {
    id: "references",
    content: `<p>
      1. Barnum, C. M. (2010). <em>Usability testing essentials : Ready, set... test!.</em> Elsevier Science & Technology.
      <br><br>2. Boren, T., & Ramey, J. (2000, October). Thinking aloud: Reconciling theory and practice. <em>IEEE Transactions on Professional Communication</em>, 43(3), 261. <a target="_blank" href="https://doi.org/10.1109/47.867942">https://doi.org/10.1109/47.867942</a>
      <br><br>3. Cirgin Ellett, M. L. DNS, RN, CGRN, & Beausang, C. C. PhD, RN. (2002). <em>Introduction to Qualitative Research. Gastroenterology Nursing</em>, 25(1), 10-14.
      <br><br>4. Crossley, N. (2005). <em>Key concepts in critical social theory.</em> SAGE Publications.
      <br><br>5. Ericsson, A. K., & Simon, H. A. (1993, April 13). <em>Protocol Analysis - Rev’d Edition: Verbal Reports as Data</em> (revised edition). Bradford Books.
      <br><br>6. Ezzy, D. (2000). Illness narratives: time, hope, and HIV. <em>Social Science and Medicine</em>, 50(5). 427-444
      <br><br>7. Ezzy, D. (2003). <em>Qualitative analysis.</em> Taylor & Francis Group.
      <br><br>8. Gray, J. H., Densten, I. L., (1998). Integrating Quantitative and Qualitative Analysis Using Latent and Manifest Variables. <em>Quality & Quantity</em> 32, 419–431. <a target="_blank" href="https://doi.org/10.1023/A:1004357719066">https://doi.org/10.1023/A:1004357719066</a>
      <br><br>9. Guest, G., MacQueen, K. M., & Namey, E. E., (2012). <em>Applied Thematic Analysis.</em> SAGE Publications Ltd.
      <br><br>10. Hartson, R., & Pyla, P. S. (2018). <em>The UX Book: Designing a Quality User Experience.</em> Elsevier Gezondheidszorg.
      <br><br>11. Hertzum, M. (2020). <em>Usability testing : A practitioner's guide to evaluating the user experience.</em> Springer International Publishing AG.
      <br><br>12. Hertzum, M., Borlund, P. and Kristoffersen, K.B. (2015). <em>What do thinking-aloud participants say? A comparison of moderated and unmoderated usability sessions. International Journal of Human-Computer Interaction</em>, 31(9), pp. 557–570. <a target="_blank" href="https://doi.org/10.1080/10447318.2015.1065691">https://doi.org/10.1080/10447318.2015.1065691</a>.
      <br><br>13. Hoehle, H., & Ventakesh, V. (2015, June). Mobile Application Usability. <em>MIS Quarterly</em>, 39(2), 435–472. <a target="_blank" href="https://www.jstor.org/stable/10.2307/26628361">https://www.jstor.org/stable/10.2307/26628361</a>
      <br><br>14. Hughes, M. (1999, November). Rigor in Usability Testing. <em>Technical Communication</em>, 46(4), 488. <a target="_blank" href="https://www.jstor.org/stable/43090399">https://www.jstor.org/stable/43090399</a>
      <br><br>15. International Organization for Standardization. (2019, July). <em>ISO 9241-210:2019</em>. ISO. Retrieved October 20, 2022, from <a target="_blank" href="https://www.iso.org/standard/77520.html">https://www.iso.org/standard/77520.html</a>
      <br><br>16. Miles, M., & Huberman, A. M. (1994). <em>Qualitative data analysis.</em> SAGE Publications.
      <br><br>17. Nielsen, J. (2000, March 18). <em>Why you only need to test with five users.</em> Nielsen Norman Group. <a target="_blank" href="https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/">https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/</a>
      <br><br>18. Nielsen, J., & Landauer, T. K. (1993). A mathematical model of the finding of usability problems. <em>In Proceedings of the INTERACT '93 and CHI '93 Conference on Human Factors in Computing Systems (CHI '93).</em> Association for Computing Machinery. 206–213. <a target="_blank" href="https://doi.org/10.1145/169059.169166">https://doi.org/10.1145/169059.169166</a>
      <br><br>19. Peacock, J. L. (1986). <em>The anthropological lens: Harsh lights, soft focus.</em> Cambridge University Press.
      <br><br>20. Rubin, J. & Chisnell, D. (2008). <em>Handbook of usability testing : How to plan, design, and conduct effective tests.</em> John Wiley & Sons, Incorporated.
      <br><br>21. Rubin, J., Chisnell, D., & Spool, J. (2011). <em>Handbook of Usability Testing</em>. John Wiley & Sons, Inc.
      <br><br>22. Segtnan, V.H., & Isaksson, T. (2000). Evaluating near Infrared Techniques for Quantitative Analysis of Carbohydrates in Fruit Juice Model Systems. <em>J. Near Infrared Spectrosc.</em> 8, 109-116 (2000)
      <br><br>23. Tamler, H. (1998, July). How (much) to intervene in a usability testing session. <em>Common Ground</em>, 8(3), 11–15. <a target="_blank" href="https://htamler.com/papers/intervene/">https://htamler.com/papers/intervene/</a>
      <br><br>24. Wand, J. N., Shotts, K. W., Sekhon, J. S., Mebane, W. R., Herron, M. C., & Brady, H. E. (2001). The Butterfly Did It: The Aberrant Vote for Buchanan in Palm Beach County, Florida. <em>The American Political Science Review</em>, 95(4), 793–810. <a target="_blank" href="http://www.jstor.org/stable/3117714">http://www.jstor.org/stable/3117714</a>
      <br><br>25. Whiteside, J., Bennett, J., & Holzblatt, K. (1988). Usability engineering: Our experience and evolution. In M. Helander (Ed.), <em>Handbook of Human-Computer Interaction</em> (pp. 791–817). Elsevier Science Publishers.
      <br><br>26. Wojnar, D. M., & Swanson, K. M. (2007). Phenomenology: An Exploration. <em>Journal of Holistic Nursing</em> 25(3), 172-180. <a target="_blank" href="https://doi.org/10.1177/0898010106295172">https://doi.org/10.1177/0898010106295172</a> 
    </p>`,
  },
];

document.addEventListener("DOMContentLoaded", () => {
  // Add data to each section
  SECTIONS.forEach((section) => {
    $(`section#${section.id}`).append(section.content);
  });

  // Set Font Size
  $("p,div.mr-auto > span").css("font-size", `${fontSize}px`);

  // Set Font Family
  $("p,a,span").css("font-family", fontFamily);

  // Refresh AOS to take into account new element positions due to dynamically added text.
  setTimeout(() => {
    AOS.refresh();
  }, 500);
});
